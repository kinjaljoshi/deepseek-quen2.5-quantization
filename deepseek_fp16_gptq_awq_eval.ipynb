{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP5JWxjyMDO0mPCHMVbynon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ec717773a404d0791427cb722e253f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_359320bbea02403495dca5800cb7cdb0",
              "IPY_MODEL_40dcd90b4d484650ad5ac0e958beb326",
              "IPY_MODEL_5067e1a294d549cfa62d6ec270958f3e"
            ],
            "layout": "IPY_MODEL_56a899a29bc74ef58dad0a669a922af8"
          }
        },
        "359320bbea02403495dca5800cb7cdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61872bd89a3d4e81a1a5ba7e4e27366f",
            "placeholder": "​",
            "style": "IPY_MODEL_cf0ee61e359243e58f006fb4f0102526",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "40dcd90b4d484650ad5ac0e958beb326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96c4ff51fdb14855a0bdd15cac49f8b6",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8812de5697ab46d080ef9e4cf6b23e39",
            "value": 4
          }
        },
        "5067e1a294d549cfa62d6ec270958f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582d7593d55e40f5862b4fb6981e6a88",
            "placeholder": "​",
            "style": "IPY_MODEL_4db90f712aff4eb2883a097126672662",
            "value": " 4/4 [00:09&lt;00:00,  2.21s/it]"
          }
        },
        "56a899a29bc74ef58dad0a669a922af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61872bd89a3d4e81a1a5ba7e4e27366f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0ee61e359243e58f006fb4f0102526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96c4ff51fdb14855a0bdd15cac49f8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8812de5697ab46d080ef9e4cf6b23e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "582d7593d55e40f5862b4fb6981e6a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db90f712aff4eb2883a097126672662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17efcca817724107a4117707a9247ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_876d69784bb44db6ba864499c87d2ad0",
              "IPY_MODEL_2a9641089b084fefb3f31951faf1b281",
              "IPY_MODEL_f85d9aba5ba14bb7af4583a373001e81"
            ],
            "layout": "IPY_MODEL_08007778709f401bb550c035fedcdedd"
          }
        },
        "876d69784bb44db6ba864499c87d2ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72daa19fbe21461791218d25b3790d72",
            "placeholder": "​",
            "style": "IPY_MODEL_5bc009ee82234dbda9345ff377f8eabd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2a9641089b084fefb3f31951faf1b281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f41aa3dd7747c189ca108bfcc95eb0",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc8d9223fab74afa9f03ccbbf9325236",
            "value": 4
          }
        },
        "f85d9aba5ba14bb7af4583a373001e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1743233a48084305a41f2d70ece75673",
            "placeholder": "​",
            "style": "IPY_MODEL_cf7fea5bebe24065b61e1b98e7a09dba",
            "value": " 4/4 [00:16&lt;00:00,  3.78s/it]"
          }
        },
        "08007778709f401bb550c035fedcdedd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72daa19fbe21461791218d25b3790d72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bc009ee82234dbda9345ff377f8eabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03f41aa3dd7747c189ca108bfcc95eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8d9223fab74afa9f03ccbbf9325236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1743233a48084305a41f2d70ece75673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7fea5bebe24065b61e1b98e7a09dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "287d342785694c72991a914456abec5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e3435840d1240f19276ca3a4f412594",
              "IPY_MODEL_12e4b45c2666409f828474fa283ef54e",
              "IPY_MODEL_460cbc63753c430a831fa5c9361c9325"
            ],
            "layout": "IPY_MODEL_5d3e7176bf014c41b9eafc7d69839b56"
          }
        },
        "6e3435840d1240f19276ca3a4f412594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1083d801823b4e28b230988b802fbb91",
            "placeholder": "​",
            "style": "IPY_MODEL_1dd2fa0f43254f82b6b0f3660350e7b1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "12e4b45c2666409f828474fa283ef54e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_920226b72ed6474e835eadd9ffd4d8ad",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1e8a74452164411894bb72bf483f0b4",
            "value": 4
          }
        },
        "460cbc63753c430a831fa5c9361c9325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfeb13188be64f7cb5041a549a5e14c4",
            "placeholder": "​",
            "style": "IPY_MODEL_aa5839746d0344f4a341e9d09e2a9b7a",
            "value": " 4/4 [00:08&lt;00:00,  2.07s/it]"
          }
        },
        "5d3e7176bf014c41b9eafc7d69839b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1083d801823b4e28b230988b802fbb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dd2fa0f43254f82b6b0f3660350e7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "920226b72ed6474e835eadd9ffd4d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e8a74452164411894bb72bf483f0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfeb13188be64f7cb5041a549a5e14c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5839746d0344f4a341e9d09e2a9b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2f4c28f555a44ea832b8892f275b45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93e713043ace490da4a55190de254b60",
              "IPY_MODEL_80e98443877b46abb09bb288cf1d8d0a",
              "IPY_MODEL_f1c877dde33f4583bd37d3b0cd8b284d"
            ],
            "layout": "IPY_MODEL_4aed8ad5f35849f89c04d842b2887e38"
          }
        },
        "93e713043ace490da4a55190de254b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a742778095df4159a9e5860000b31c02",
            "placeholder": "​",
            "style": "IPY_MODEL_e15057e656ea4f0496df1c6e70e8f697",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "80e98443877b46abb09bb288cf1d8d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69702b815c034cb78cea0142da9efd8c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f5bfa6723ab4844b265aebe296b5de5",
            "value": 4
          }
        },
        "f1c877dde33f4583bd37d3b0cd8b284d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70d9852f8d3a4e759d565bfb5f611980",
            "placeholder": "​",
            "style": "IPY_MODEL_c468c483eee146f6add8e56d6f4031b5",
            "value": " 4/4 [00:17&lt;00:00,  3.85s/it]"
          }
        },
        "4aed8ad5f35849f89c04d842b2887e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a742778095df4159a9e5860000b31c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e15057e656ea4f0496df1c6e70e8f697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69702b815c034cb78cea0142da9efd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f5bfa6723ab4844b265aebe296b5de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70d9852f8d3a4e759d565bfb5f611980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c468c483eee146f6add8e56d6f4031b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinjaljoshi/deepseek-quen2.5-quantization/blob/master/deepseek_fp16_gptq_awq_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluates the [**DeepSeek-R1-Distill-Qwen-14B**](https://huggingface.co/Qwen/Qwen2.5-14B) model across different precision levels **(FP16, 8-bit, 4-bit, GPTQ, AWQ, and offloaded)** by testing its performance on simple and complex tasks.\n",
        "* Simple tasks include fact questions, math problems, and science explanations\n",
        "* Complex tasks require multi-step reasoning such as classification, phishing detection, and fraud analysis.\n",
        "\n",
        "The model is benchmarked using default generation settings, a ** sampling strategy (temperature=0.6, top-p=0.95, 20 responses)**, and **beam search (beam sizes 2-5)**.\n",
        "\n",
        "Inference time is measured for each configuration, and results are stored in a structured DataFrame for comparison. The final analysis provides insights into the trade-offs between speed, accuracy, and computational efficiency across various quantization methods and decoding strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4dJkPOWLSmbB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "suObKclvE_Iq"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "cHdczBeVewR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U autoawq"
      ],
      "metadata": {
        "id": "GAGBVolcnOc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import psutil\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AwqConfig\n",
        "import time\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from accelerate import dispatch_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ufyl-71lzkv",
        "outputId": "5020dec3-330d-4f0b-e142-44275b7d69d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Displays CPU and GPU memory usage.\"\"\"\n",
        "    print(f\"\\n {'+' *5}Memory Usage{'+' *5}\")\n",
        "    print(f\"CPU Memory: {psutil.virtual_memory().percent}% used\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")\n",
        "\n",
        "def load_model(model_name, precision, offload=False):\n",
        "    \"\"\"Load DeepSeek-R1-Distill-Qwen-14B model in given precision.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    if precision == \"full\":\n",
        "      #loading FP32 will require more than 40GM of memory so split 70% GPU and 30% CPU\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "        # Get total number of layers\n",
        "        num_layers = len(model.model.layers)\n",
        "        gpu_layers = int(0.7 * num_layers)  # Assign 70% layers to GPU\n",
        "        cpu_layers = num_layers - gpu_layers  # Remaining 30% to CPU\n",
        "\n",
        "        # Create device map\n",
        "        device_map = {f\"model.layers.{i}\": \"cuda\" if i < gpu_layers else \"cpu\"\n",
        "                      for i in range(num_layers)}\n",
        "        device_map[\"lm_head\"] = \"cuda\"  # Keep final output head on GPU\n",
        "\n",
        "        # Dispatch model across CPU and GPU\n",
        "        model = dispatch_model(model, device_map=device_map)\n",
        "        print(f\"Assigned {gpu_layers} layers to GPU and {cpu_layers} layers to CPU.\")\n",
        "\n",
        "    elif precision == \"FP16\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    elif precision == \"8bit\":\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "        )\n",
        "        #.to('cuda') is not needed as model is automatically moved to device when using bitsandbytes\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config)\n",
        "    elif precision == \"4bit\":\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,  # Enable 4-bit quantization\n",
        "                    bnb_4bit_compute_dtype=torch.float16,  # Match input dtype for faster inference\n",
        "                    bnb_4bit_use_double_quant=True  # Further compression optimization\n",
        "                    )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, quantization_config=bnb_config,device_map=\"auto\")\n",
        "    elif precision == \"gptq\":\n",
        "        model = AutoGPTQForCausalLM.from_pretrained(model_name, quantize_config={\"bits\": 4}).to(\"cuda\")\n",
        "    elif precision == \"awq\":\n",
        "        awq_config = AwqConfig(\n",
        "                                          bits=4,\n",
        "                                          fuse_max_seq_len=512,\n",
        "                                          do_fuse=True,\n",
        "                                        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=awq_config).to(\"cuda\")\n",
        "\n",
        "    if offload:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "        # Get total number of layers\n",
        "        num_layers = len(model.model.layers)\n",
        "        gpu_layers = int(0.5 * num_layers)  # Assign 70% layers to GPU\n",
        "        cpu_layers = num_layers - gpu_layers  # Remaining 30% to CPU\n",
        "\n",
        "        # Create device map\n",
        "        device_map = {f\"model.layers.{i}\": \"cuda\" if i < gpu_layers else \"cpu\"\n",
        "                      for i in range(num_layers)}\n",
        "        device_map[\"lm_head\"] = \"cuda\"  # Keep final output head on GPU\n",
        "\n",
        "        # Dispatch model across CPU and GPU\n",
        "        model = dispatch_model(model, device_map=device_map)\n",
        "        print(f\"Assigned {gpu_layers} layers to GPU and {cpu_layers} layers to CPU.\")\n",
        "\n",
        "    model.eval()\n",
        "    get_memory_usage()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "5U2xhF7zZmPS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_tasks = [\n",
        "    \"What is the capital of France, just provide answer, explanation is not needed?\",\n",
        "    \"Who wrote 'Lord of the rings, provide name of autho, explanation is not needed?\",\n",
        "    \"What is the boiling point of water in degree centigrade, provide answer, explanantion is not needed?\",\n",
        "    \"Solve for x: 5x + 10 = 30, solve for x and explanation is not needed\",\n",
        "    \"Explain the process of photosynthesis in one sentence.\"\n",
        "]\n",
        "\n",
        "complex_tasks = [\n",
        "    \"You are an AI assistant. First, classify the user's request as either 'math', 'science', or 'general'. \"\n",
        "    \"Then, if it's 'math', solve the problem. If it's 'science', explain the concept. If it's 'general', answer normally. \"\n",
        "    \"User request: 'What is the integral of x^2 + 3x + 5?'\",\n",
        "\n",
        "    \"A user provides an email and asks whether it's a phishing attempt. First, classify the email as 'safe' or 'suspicious'. \"\n",
        "    \"Then, if 'suspicious', list the warning signs. If 'safe', explain why it's trustworthy. Email: 'Dear user, your bank account has been compromised. Click this link to secure it now.'\",\n",
        "\n",
        "    \"A company wants to classify customer feedback as 'positive', 'neutral', or 'negative'. Once classified, provide a response appropriate for the sentiment. \"\n",
        "    \"Customer feedback: 'The delivery was delayed by 5 days and no updates were given. I'm very disappointed.'\",\n",
        "\n",
        "    \"Given a dataset with thousands of financial transactions, design a multi-step approach to detect fraud. \"\n",
        "    \"Explain how you would preprocess the data, extract features, and implement an anomaly detection system.\",\n",
        "\n",
        "    \"You are a python expert, provide python only in response no explanation is needed.Write a Python function to calculate the factorial of a given number.\",\n",
        "\n",
        "    \"You are a python expert, provide python only in response no explanation is needed.Generate a Python script that reads a CSV file and calculates the average value of a specific column.\",\n",
        "\n",
        "    \"You are a python expert, provide python only in response no explanation is needed.Write a Python class named `BankAccount` with methods for deposit, withdrawal, and balance check.\",\n",
        "\n",
        "    \"You are a python expert, provide python only in response no explanation is needed.Create a Python program that fetches real-time weather data using an API and prints the temperature and humidity.\",\n",
        "\n",
        "    \"\"\"You are a SQL expert, provide SQL Query only in response no explanation is needed.\n",
        "     Question - Given the following table, write an SQL query to find all employees who have been with the company for more than 5 years:\n",
        "     Table: Employees\n",
        "     Columns: employee_id (INT), name (VARCHAR), hire_date (DATE), department (VARCHAR)\"\"\",\n",
        "\n",
        "    \"\"\"Given the following two tables, write an SQL query to find the total sales per customer:\n",
        "     Table: Customers\n",
        "     Columns: customer_id (INT), name (VARCHAR), email (VARCHAR)\n",
        "     Table: Orders\n",
        "     Columns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (DECIMAL)\"\"\",\n",
        "\n",
        "    \"\"\"You are a SQL expert, provide SQL Query only in response no explanation is needed.\n",
        "      Question - Given the following three tables, write an SQL query to find the top 3 selling products in the last month:\n",
        "     Table: Products\n",
        "     Columns: product_id (INT), name (VARCHAR), price (DECIMAL)\n",
        "     Table: Orders\n",
        "     Columns: order_id (INT), customer_id (INT), order_date (DATE)\n",
        "     Table: Order_Items\n",
        "     Columns: order_item_id (INT), order_id (INT), product_id (INT), quantity (INT)\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "gFBl1Jlle0L5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_config = {\n",
        "    \"do_sample\":True,\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.9,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"max_new_tokens\": 512\n",
        "}\n",
        "\n",
        "beam_sizes = [2]\n",
        "\n",
        "#sequential run\n",
        "def run_task(model, tokenizer, prompt, config=None):\n",
        "    \"\"\"Runs inference and measures time taken.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    start = time.time()\n",
        "    if config:\n",
        "        output = model.generate(**inputs, **config, eos_token_id=tokenizer.eos_token_id,\n",
        "                                pad_token_id=tokenizer.eos_token_id)\n",
        "    else:\n",
        "        output = model.generate(**inputs, max_new_tokens=512,eos_token_id=tokenizer.eos_token_id,\n",
        "                                pad_token_id=tokenizer.eos_token_id)\n",
        "    end = time.time()\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return  response, end - start\n",
        "#batch run\n",
        "def run_batch_task(model, tokenizer, prompts, config=None):\n",
        "\n",
        "    #define system prompt\n",
        "    system_message = \"System: You are an AI assistant. Please provide clear, concise, and accurate responses.\\nUser: \"\n",
        "    updated_prompts = [system_message + prompt for prompt in prompts]\n",
        "\n",
        "    inputs = tokenizer(updated_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    start = time.time()\n",
        "    if config:\n",
        "        output = model.generate(**inputs, **config)\n",
        "    else:\n",
        "        output = model.generate(**inputs, max_new_tokens=512)\n",
        "    end = time.time()\n",
        "    responses = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
        "    return responses, end - start"
      ],
      "metadata": {
        "id": "oOQ7lRq2e3Rh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP16"
      ],
      "metadata": {
        "id": "RcBLplVfo-8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = \"FP16\"\n",
        "offload =  False\n",
        "model, tokenizer = load_model(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", precision if precision != \"offloaded\" else \"fp16\", offload)\n",
        "\n",
        "\n",
        "\n",
        "def test_16bit_model_batch(model, tokenizer):\n",
        "    results = []\n",
        "    print(f\"\\n {'+' *5} Testing {precision} Precision {'+' *5}\")\n",
        "\n",
        "    # Run simple tasks in batch (Default)\n",
        "    simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks)\n",
        "    print(f\"Batch Simple Task Time (Default): {simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Default\", prompt, simple_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run complex tasks in batch (Default)\n",
        "    complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks)\n",
        "    print(f\"Batch Complex Task Time (Default): {complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Default\", prompt, complex_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run simple tasks with benchmark configuration\n",
        "    benchmark_simple_responses, benchmark_simple_time = run_batch_task(model, tokenizer, simple_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Simple Task Time (Benchmark): {benchmark_simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, benchmark_simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Benchmark\", prompt, \"-\", benchmark_simple_time, \"-\", response))\n",
        "\n",
        "    # Run complex tasks with benchmark configuration\n",
        "    benchmark_complex_responses, benchmark_complex_time = run_batch_task(model, tokenizer, complex_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Complex Task Time (Benchmark): {benchmark_complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, benchmark_complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Benchmark\", prompt, \"-\", benchmark_complex_time, \"-\", response))\n",
        "\n",
        "    # Run simple tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks, config=beam_config)\n",
        "        print(f\"Batch Simple Task Time (Beam {beam_size}): {simple_time:.2f}s\")\n",
        "        for prompt, response in zip(simple_tasks, simple_responses):\n",
        "            results.append((precision, \"Batch Simple\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", simple_time, response))\n",
        "\n",
        "    # Run complex tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks, config=beam_config)\n",
        "        print(f\"Batch Complex Task Time (Beam {beam_size}): {complex_time:.2f}s\")\n",
        "        for prompt, response in zip(complex_tasks, complex_responses):\n",
        "            results.append((precision, \"Batch Complex\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", complex_time, response))\n",
        "\n",
        "    # Convert results to DataFrame for analysis\n",
        "    df = pd.DataFrame(results, columns=[\"Precision\", \"Batch Type\", \"Method\", \"Task\", \"Default Time\", \"Benchmark Time\", \"Beam Search Time\", \"Response\"])\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# df_results16_bits = test_16bit_model(model, tokenizer)\n",
        "df_results_batch = test_16bit_model_batch(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "4ec717773a404d0791427cb722e253f2",
            "359320bbea02403495dca5800cb7cdb0",
            "40dcd90b4d484650ad5ac0e958beb326",
            "5067e1a294d549cfa62d6ec270958f3e",
            "56a899a29bc74ef58dad0a669a922af8",
            "61872bd89a3d4e81a1a5ba7e4e27366f",
            "cf0ee61e359243e58f006fb4f0102526",
            "96c4ff51fdb14855a0bdd15cac49f8b6",
            "8812de5697ab46d080ef9e4cf6b23e39",
            "582d7593d55e40f5862b4fb6981e6a88",
            "4db90f712aff4eb2883a097126672662"
          ]
        },
        "id": "U42LBepWfCNA",
        "outputId": "6853937b-07f0-42d6-e6cb-8d3adf9723fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ec717773a404d0791427cb722e253f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " +++++Memory Usage+++++\n",
            "CPU Memory: 4.9% used\n",
            "GPU Memory: 29.69 GB reserved\n",
            "\n",
            " +++++ Testing FP16 Precision +++++\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Default): 15.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Default): 33.75s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Benchmark): 10.90s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Benchmark): 33.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Beam 2): 25.78s\n",
            "Batch Complex Task Time (Beam 2): 43.45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_batch.to_csv(\"test_result_16bits.csv\", index=False)"
      ],
      "metadata": {
        "id": "Qn_SJwDF-A5q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FP8"
      ],
      "metadata": {
        "id": "am3Du85XpCrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = \"8bit\"\n",
        "offload =  False\n",
        "model, tokenizer = load_model(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", precision, offload)\n",
        "\n",
        "\n",
        "\n",
        "def test_8bits_model_batch(model, tokenizer):\n",
        "    results = []\n",
        "    print(f\"\\n {'+' *5} Testing {precision} Precision {'+' *5}\")\n",
        "\n",
        "    # Run simple tasks in batch (Default)\n",
        "    simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks)\n",
        "    print(f\"Batch Simple Task Time (Default): {simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Default\", prompt, simple_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run complex tasks in batch (Default)\n",
        "    complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks)\n",
        "    print(f\"Batch Complex Task Time (Default): {complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Default\", prompt, complex_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run simple tasks with benchmark configuration\n",
        "    benchmark_simple_responses, benchmark_simple_time = run_batch_task(model, tokenizer, simple_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Simple Task Time (Benchmark): {benchmark_simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, benchmark_simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Benchmark\", prompt, \"-\", benchmark_simple_time, \"-\", response))\n",
        "\n",
        "    # Run complex tasks with benchmark configuration\n",
        "    benchmark_complex_responses, benchmark_complex_time = run_batch_task(model, tokenizer, complex_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Complex Task Time (Benchmark): {benchmark_complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, benchmark_complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Benchmark\", prompt, \"-\", benchmark_complex_time, \"-\", response))\n",
        "\n",
        "    # Run simple tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks, config=beam_config)\n",
        "        print(f\"Batch Simple Task Time (Beam {beam_size}): {simple_time:.2f}s\")\n",
        "        for prompt, response in zip(simple_tasks, simple_responses):\n",
        "            results.append((precision, \"Batch Simple\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", simple_time, response))\n",
        "\n",
        "    # Run complex tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks, config=beam_config)\n",
        "        print(f\"Batch Complex Task Time (Beam {beam_size}): {complex_time:.2f}s\")\n",
        "        for prompt, response in zip(complex_tasks, complex_responses):\n",
        "            results.append((precision, \"Batch Complex\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", complex_time, response))\n",
        "\n",
        "    # Convert results to DataFrame for analysis\n",
        "    df = pd.DataFrame(results, columns=[\"Precision\", \"Batch Type\", \"Method\", \"Task\", \"Default Time\", \"Benchmark Time\", \"Beam Search Time\", \"Response\"])\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_results_batch = test_8bits_model_batch(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483,
          "referenced_widgets": [
            "17efcca817724107a4117707a9247ef0",
            "876d69784bb44db6ba864499c87d2ad0",
            "2a9641089b084fefb3f31951faf1b281",
            "f85d9aba5ba14bb7af4583a373001e81",
            "08007778709f401bb550c035fedcdedd",
            "72daa19fbe21461791218d25b3790d72",
            "5bc009ee82234dbda9345ff377f8eabd",
            "03f41aa3dd7747c189ca108bfcc95eb0",
            "bc8d9223fab74afa9f03ccbbf9325236",
            "1743233a48084305a41f2d70ece75673",
            "cf7fea5bebe24065b61e1b98e7a09dba"
          ]
        },
        "id": "Qzx8ACYDogCQ",
        "outputId": "56fc2bb6-54f2-4764-b907-9756f83f4187"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17efcca817724107a4117707a9247ef0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " +++++Memory Usage+++++\n",
            "CPU Memory: 4.9% used\n",
            "GPU Memory: 16.54 GB reserved\n",
            "\n",
            " +++++ Testing 8bit Precision +++++\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Default): 42.71s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Default): 131.79s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Benchmark): 103.14s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Benchmark): 130.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Beam 2): 102.34s\n",
            "Batch Complex Task Time (Beam 2): 151.97s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_batch.to_csv(\"test_result_8bits.csv\", index=False)"
      ],
      "metadata": {
        "id": "EgYrDQezoEot"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gptq"
      ],
      "metadata": {
        "id": "X33uXenypGke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = \"gptq\"\n",
        "offload =  False\n",
        "model, tokenizer = load_model(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", precision, offload)\n",
        "\n",
        "\n",
        "\n",
        "def test_gptq_model_batch(model, tokenizer):\n",
        "    results = []\n",
        "    print(f\"\\n {'+' *5} Testing {precision} Precision {'+' *5}\")\n",
        "\n",
        "    # Run simple tasks in batch (Default)\n",
        "    simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks)\n",
        "    print(f\"Batch Simple Task Time (Default): {simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Default\", prompt, simple_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run complex tasks in batch (Default)\n",
        "    complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks)\n",
        "    print(f\"Batch Complex Task Time (Default): {complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Default\", prompt, complex_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run simple tasks with benchmark configuration\n",
        "    benchmark_simple_responses, benchmark_simple_time = run_batch_task(model, tokenizer, simple_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Simple Task Time (Benchmark): {benchmark_simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, benchmark_simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Benchmark\", prompt, \"-\", benchmark_simple_time, \"-\", response))\n",
        "\n",
        "    # Run complex tasks with benchmark configuration\n",
        "    benchmark_complex_responses, benchmark_complex_time = run_batch_task(model, tokenizer, complex_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Complex Task Time (Benchmark): {benchmark_complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, benchmark_complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Benchmark\", prompt, \"-\", benchmark_complex_time, \"-\", response))\n",
        "\n",
        "    # Run simple tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks, config=beam_config)\n",
        "        print(f\"Batch Simple Task Time (Beam {beam_size}): {simple_time:.2f}s\")\n",
        "        for prompt, response in zip(simple_tasks, simple_responses):\n",
        "            results.append((precision, \"Batch Simple\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", simple_time, response))\n",
        "\n",
        "    # Run complex tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks, config=beam_config)\n",
        "        print(f\"Batch Complex Task Time (Beam {beam_size}): {complex_time:.2f}s\")\n",
        "        for prompt, response in zip(complex_tasks, complex_responses):\n",
        "            results.append((precision, \"Batch Complex\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", complex_time, response))\n",
        "\n",
        "    # Convert results to DataFrame for analysis\n",
        "    df = pd.DataFrame(results, columns=[\"Precision\", \"Batch Type\", \"Method\", \"Task\", \"Default Time\", \"Benchmark Time\", \"Beam Search Time\", \"Response\"])\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_results_batch = test_gptq_model_batch(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "287d342785694c72991a914456abec5e",
            "6e3435840d1240f19276ca3a4f412594",
            "12e4b45c2666409f828474fa283ef54e",
            "460cbc63753c430a831fa5c9361c9325",
            "5d3e7176bf014c41b9eafc7d69839b56",
            "1083d801823b4e28b230988b802fbb91",
            "1dd2fa0f43254f82b6b0f3660350e7b1",
            "920226b72ed6474e835eadd9ffd4d8ad",
            "c1e8a74452164411894bb72bf483f0b4",
            "cfeb13188be64f7cb5041a549a5e14c4",
            "aa5839746d0344f4a341e9d09e2a9b7a"
          ]
        },
        "id": "nE-SVZKR0vdf",
        "outputId": "46f34507-5e65-4a66-9868-aecaa60f0606"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "287d342785694c72991a914456abec5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " +++++Memory Usage+++++\n",
            "CPU Memory: 5.3% used\n",
            "GPU Memory: 29.69 GB reserved\n",
            "\n",
            " +++++ Testing gptq Precision +++++\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Default): 9.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Default): 29.63s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Benchmark): 28.91s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Benchmark): 29.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Beam 2): 29.22s\n",
            "Batch Complex Task Time (Beam 2): 40.99s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_batch.to_csv(\"test_result_gptq.csv\", index=False)"
      ],
      "metadata": {
        "id": "Lg89WOwOmZOG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4bits\n",
        "\n",
        "**UserWarning**: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default).\n",
        "This will lead to slow inference or training speed.\n",
        "\n",
        "\n",
        "This warning means that:\n",
        "Model is receiving inputs in torch.float16 (which is more efficient for GPU computation).\n",
        "However, bitsandbytes (used for 4-bit quantization) is processing computations in torch.float32 by default.\n",
        "This mismatch in data types can slow down inference/training because:\n",
        "1. The model has to convert data between float16 and float32, which adds computational overhead.\n",
        "2. float16 is faster on GPUs (especially NVIDIA Tensor Cores), whereas float32 takes more memory and is slower.\n"
      ],
      "metadata": {
        "id": "7wjP-g-TpN5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = \"4bit\"\n",
        "offload =  False\n",
        "model, tokenizer = load_model(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", precision, offload)\n",
        "\n",
        "\n",
        "\n",
        "def test_gptq_model_batch(model, tokenizer):\n",
        "    results = []\n",
        "    print(f\"\\n {'+' *5} Testing {precision} Precision {'+' *5}\")\n",
        "\n",
        "    # Run simple tasks in batch (Default)\n",
        "    simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks)\n",
        "    print(f\"Batch Simple Task Time (Default): {simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Default\", prompt, simple_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run complex tasks in batch (Default)\n",
        "    complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks)\n",
        "    print(f\"Batch Complex Task Time (Default): {complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Default\", prompt, complex_time, \"-\", \"-\", response))\n",
        "\n",
        "    # Run simple tasks with benchmark configuration\n",
        "    benchmark_simple_responses, benchmark_simple_time = run_batch_task(model, tokenizer, simple_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Simple Task Time (Benchmark): {benchmark_simple_time:.2f}s\")\n",
        "    for prompt, response in zip(simple_tasks, benchmark_simple_responses):\n",
        "        results.append((precision, \"Batch Simple\", \"Benchmark\", prompt, \"-\", benchmark_simple_time, \"-\", response))\n",
        "\n",
        "    # Run complex tasks with benchmark configuration\n",
        "    benchmark_complex_responses, benchmark_complex_time = run_batch_task(model, tokenizer, complex_tasks, config=benchmark_config)\n",
        "    print(f\"Batch Complex Task Time (Benchmark): {benchmark_complex_time:.2f}s\")\n",
        "    for prompt, response in zip(complex_tasks, benchmark_complex_responses):\n",
        "        results.append((precision, \"Batch Complex\", \"Benchmark\", prompt, \"-\", benchmark_complex_time, \"-\", response))\n",
        "\n",
        "    # Run simple tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        simple_responses, simple_time = run_batch_task(model, tokenizer, simple_tasks, config=beam_config)\n",
        "        print(f\"Batch Simple Task Time (Beam {beam_size}): {simple_time:.2f}s\")\n",
        "        for prompt, response in zip(simple_tasks, simple_responses):\n",
        "            results.append((precision, \"Batch Simple\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", simple_time, response))\n",
        "\n",
        "    # Run complex tasks with beam search\n",
        "    for beam_size in beam_sizes:\n",
        "        beam_config = {\"num_beams\": beam_size, \"max_new_tokens\": 512}\n",
        "        complex_responses, complex_time = run_batch_task(model, tokenizer, complex_tasks, config=beam_config)\n",
        "        print(f\"Batch Complex Task Time (Beam {beam_size}): {complex_time:.2f}s\")\n",
        "        for prompt, response in zip(complex_tasks, complex_responses):\n",
        "            results.append((precision, \"Batch Complex\", f\"Beam {beam_size}\", prompt, \"-\", \"-\", complex_time, response))\n",
        "\n",
        "    # Convert results to DataFrame for analysis\n",
        "    df = pd.DataFrame(results, columns=[\"Precision\", \"Batch Type\", \"Method\", \"Task\", \"Default Time\", \"Benchmark Time\", \"Beam Search Time\", \"Response\"])\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_results_batch = test_gptq_model_batch(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "b2f4c28f555a44ea832b8892f275b45a",
            "93e713043ace490da4a55190de254b60",
            "80e98443877b46abb09bb288cf1d8d0a",
            "f1c877dde33f4583bd37d3b0cd8b284d",
            "4aed8ad5f35849f89c04d842b2887e38",
            "a742778095df4159a9e5860000b31c02",
            "e15057e656ea4f0496df1c6e70e8f697",
            "69702b815c034cb78cea0142da9efd8c",
            "9f5bfa6723ab4844b265aebe296b5de5",
            "70d9852f8d3a4e759d565bfb5f611980",
            "c468c483eee146f6add8e56d6f4031b5"
          ]
        },
        "id": "LdYGpx12mmDO",
        "outputId": "6cecce62-2c6a-474d-c3c7-31e35e419408"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f4c28f555a44ea832b8892f275b45a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " +++++Memory Usage+++++\n",
            "CPU Memory: 4.6% used\n",
            "GPU Memory: 10.03 GB reserved\n",
            "\n",
            " +++++ Testing 4bit Precision +++++\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Default): 51.32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Default): 61.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Benchmark): 37.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Complex Task Time (Benchmark): 62.54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Simple Task Time (Beam 2): 59.74s\n",
            "Batch Complex Task Time (Beam 2): 69.87s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_batch.to_csv(\"test_result_4bits.csv\", index=False)"
      ],
      "metadata": {
        "id": "xiwmMa2jrq9M"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}